{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Model Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores **building the fraud detection model**.\n",
    "This model will learn what \"normal\" transactions are like, and will judge whether a new transaction is different enough from a \"normal\" one that we should consider it fraudulent.\n",
    "\n",
    "The following work will look fairly standard to anyone having trained machine learning models using python Jupyter notebooks.\n",
    "The CML platform provides a **fully capable Jupyter notebook environment** that data scientists know and love.\n",
    "\n",
    "In this part, you will learn how to train ML models using pandas and both scikit-learn's OneClassSVM and an autoencoder using Pytorch. For more detailed information on that approaches used here, please see Cloudera Fast Forward's [Deep Learning for Anomaly Detection](https://ff12.fastforwardlabs.com/) report.\n",
    "\n",
    "If you haven't yet, run through the initialization steps in the README file and Part 1. \n",
    "In Part 1, the data is imported into the `default.cc_data` table in Hive. \n",
    "All data accesses fetch from Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Kaggle [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset here. The datasets contains transactions made by credit cards in September 2013 by European cardholders, and presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "The dataset should be uploaded to hive in the step 1. We will first load it as a Pandas DataFrame via Spark, and check the number of positive and negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: decimal(10,0) (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonSQL\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    spark_df = spark.sql(\"SELECT * FROM default.cc_data\")\n",
    "    spark_df.printSchema()\n",
    "    data = spark_df.toPandas()\n",
    "except:\n",
    "    data = pd.read_csv(\"/home/cdsw/data/creditcardfraud.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 284807\n",
      "Positive samples: 284315\n",
      "Negative samples: 492\n"
     ]
    }
   ],
   "source": [
    "print('Number of records:',len(data))\n",
    "print('Positive samples:',len(data[data.Class==0]))\n",
    "print('Negative samples:',len(data[data.Class==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains only numerical input variables which are the result of a PCA transformation. Features V1, V2, … V28 are the principal components obtained using a PCA process, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, and the feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0    0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1    0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2    1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3    1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4    2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Choosing positive samples\n",
    "\n",
    "The next step is to munge the data for later steps. \n",
    "As we saw, this dataset has already been preprocessed to be very clean, and entirely numeric.\n",
    "The only column that needs cleaning is `Class`.\n",
    "For anomaly detection, scikit-learn marks inliers as `1`, and outliers as `-1`,\n",
    "whereas this dataset has marked them `0` and `1`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Class'] = data['Class'].replace({0: 1, 1: -1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoder for fraud detection uses only the positive (non-fraud) samples to train the model, thus only chooses records with Class==0. Then we split the chosen records as train set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In class training set: 284315\n"
     ]
    }
   ],
   "source": [
    "in_class_set = data[data.Class==1]\n",
    "out_class_set = data[data.Class==-1]\n",
    "print('In class training set:',len(in_class_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Class SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the main model used in this project is a deep learning model, there are other techniques that can be used for anomaly detection. Here we show how to use a scikit-learn OneClassSVM.\n",
    "\n",
    "This step follows a fairly standard ML workflow. Here, we:\n",
    "- normalize the numeric features\n",
    "- split into *test* and *train* sets\n",
    "- train a classification model using these processed features\n",
    "- evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For anomaly detection, we train the model *only* on inliers so it learns what normal data look like.\n",
    "We then test on both inliers and outliers, so splitting the *test* and *train* sets will be slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test/train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(in_class_set.drop('Class', axis=1),\n",
    "                                                     in_class_set['Class'],\n",
    "                                                     test_size=len(out_class_set),  # balanced test set\n",
    "                                                     random_state=42)\n",
    "# add outliers to test set\n",
    "X_test = X_test.append(out_class_set.drop('Class', axis=1))\n",
    "y_test = y_test.append(out_class_set['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features based on test set\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneClassSVM(gamma=0.5, nu=0.01, verbose=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train one-class SVM\n",
    "clf = OneClassSVM(gamma=0.5, nu=0.01, verbose=True)\n",
    "clf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9900008103642058\n",
      "test accuracy: 0.8404471544715447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.69      0.81       492\n",
      "           1       0.76      0.99      0.86       492\n",
      "\n",
      "    accuracy                           0.84       984\n",
      "   macro avg       0.87      0.84      0.84       984\n",
      "weighted avg       0.87      0.84      0.84       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate one-class SVM\n",
    "print('train accuracy:', accuracy_score(y_train, clf.predict(X_train)))\n",
    "print('test accuracy:', accuracy_score(y_test, clf.predict(X_test)))\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb05655eb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ocsvm_auc = roc_auc_score(y_test, clf.score_samples(X_test))\n",
    "fpr, tpr, _ = roc_curve(y_test, clf.score_samples(X_test))\n",
    "plt.plot(fpr, tpr, label='OCSVM')\n",
    "plt.title('OCSVM: AUROC=%.3f' % (ocsvm_auc))\n",
    "plt.ylabel('true positive rate')\n",
    "plt.xlabel('false positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows how we can train a deep learning model using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the model, we will use the full dataset and not a balanced set like we did for the OneClassSVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "feature_names=data.columns.values[:-1]\n",
    "train_test_set = in_class_set[feature_names]\n",
    "train_set, test_set = train_test_split(train_test_set, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.42327013e-01, 9.47416596e-01, 7.97217483e-01, 8.81816356e-01,\n",
       "        2.41740662e-01, 5.43571003e-01, 5.15661441e-01, 4.19171267e-01,\n",
       "        8.02040109e-01, 4.16891659e-01, 4.47903682e-01, 2.35464057e-01,\n",
       "        6.15377573e-01, 4.10107015e-01, 5.23937212e-01, 4.48405051e-01,\n",
       "        6.02247154e-01, 6.71423155e-01, 6.12703253e-01, 5.13449227e-01,\n",
       "        4.11165178e-01, 6.10928994e-01, 5.39959714e-01, 6.67924552e-01,\n",
       "        5.06573308e-01, 5.33984885e-01, 5.03033612e-01, 4.65364700e-01,\n",
       "        2.22235356e-01, 5.09856012e-03],\n",
       "       [2.44658194e-01, 9.79156142e-01, 7.97062325e-01, 8.90512717e-01,\n",
       "        3.03888114e-01, 5.48372609e-01, 5.15350500e-01, 4.21609233e-01,\n",
       "        7.95745386e-01, 3.66345976e-01, 4.95729107e-01, 3.75257082e-01,\n",
       "        6.96864586e-01, 5.41487510e-01, 5.84645532e-01, 3.64243237e-01,\n",
       "        5.45108394e-01, 6.30551391e-01, 5.31712561e-01, 5.85659779e-01,\n",
       "        4.13125001e-01, 6.06201154e-01, 5.15266633e-01, 6.62486089e-01,\n",
       "        4.14903779e-01, 6.25814080e-01, 3.86877333e-01, 4.70760274e-01,\n",
       "        2.21269048e-01, 5.08736791e-05]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler=MinMaxScaler().fit(train_set)\n",
    "train_set=scaler.transform(train_set)\n",
    "test_set=scaler.transform(test_set)\n",
    "train_set[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the distribution of each feature. The following diagram shows the distribution of the first 4 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb055e716d8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb055ea0c18>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.Series(train_set[:,0]).hist(bins=100)\n",
    "pd.Series(train_set[:,1]).hist(bins=100)\n",
    "pd.Series(train_set[:,2]).hist(bins=100)\n",
    "pd.Series(train_set[:,3]).hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "Here we define the autoencoder model for PyTorch. There are 2 hidden layers each for the encoder and decoder, with 15 and 7 cells respectively, and ReLU is used as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self,num_input):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_input, 15),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(15, 7))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(7, 15),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(15, num_input),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this process work with both CPU and GPU sessions, the GPU specific requirements are put after a `if torch.cuda.is_available():` check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# pytorch parallelism does not work as expected when using GPU.\n",
    "if torch.cuda.is_available():\n",
    "    num_workers = 4\n",
    "else:\n",
    "    num_workers = 0\n",
    "\n",
    "inputs = torch.tensor(train_set, dtype=torch.float32)\n",
    "dataset = TensorDataset(inputs)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "model = autoencoder(inputs.shape[1])\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda') \n",
    "    criterion.to('cuda')\n",
    "    inputs.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining test function\n",
    "\n",
    "The test function is used for cross validation during the training. Here we choose to do cross validation for each 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tests_=torch.tensor(test_set, dtype=torch.float32)\n",
    "        if torch.cuda.is_available():\n",
    "            tests_.to('cuda')\n",
    "        outputs = model(tests_)\n",
    "        loss=criterion(outputs,tests_)\n",
    "    return loss.item()/(tests_.shape[0]*tests_.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-15 09:22:57.996749 epoch [5/100], loss:0.001131, test_set_loss:0.001100\n",
      "2020-07-15 09:23:12.691952 epoch [10/100], loss:0.001023, test_set_loss:0.001017\n",
      "2020-07-15 09:23:27.247957 epoch [15/100], loss:0.001015, test_set_loss:0.001007\n",
      "2020-07-15 09:23:40.174849 epoch [20/100], loss:0.001015, test_set_loss:0.000996\n",
      "2020-07-15 09:23:54.091424 epoch [25/100], loss:0.000998, test_set_loss:0.001002\n",
      "2020-07-15 09:24:07.949289 epoch [30/100], loss:0.001051, test_set_loss:0.002114\n",
      "2020-07-15 09:24:23.586194 epoch [35/100], loss:0.001413, test_set_loss:0.001400\n",
      "2020-07-15 09:24:37.028436 epoch [40/100], loss:0.001370, test_set_loss:0.001373\n",
      "2020-07-15 09:24:50.408207 epoch [45/100], loss:0.001443, test_set_loss:0.001436\n",
      "2020-07-15 09:25:04.626012 epoch [50/100], loss:0.001980, test_set_loss:0.001960\n",
      "2020-07-15 09:25:20.305788 epoch [55/100], loss:0.001706, test_set_loss:0.001700\n",
      "2020-07-15 09:25:34.430974 epoch [60/100], loss:0.001655, test_set_loss:0.001651\n",
      "2020-07-15 09:25:47.994207 epoch [65/100], loss:0.001708, test_set_loss:0.001697\n",
      "2020-07-15 09:26:02.165392 epoch [70/100], loss:0.001694, test_set_loss:0.001693\n",
      "2020-07-15 09:26:17.061397 epoch [75/100], loss:0.001692, test_set_loss:0.001686\n",
      "2020-07-15 09:26:32.543940 epoch [80/100], loss:0.001690, test_set_loss:0.001690\n",
      "2020-07-15 09:26:49.120109 epoch [85/100], loss:0.001690, test_set_loss:0.001689\n",
      "2020-07-15 09:27:04.576103 epoch [90/100], loss:0.001689, test_set_loss:0.001690\n",
      "2020-07-15 09:27:21.265877 epoch [95/100], loss:0.001689, test_set_loss:0.001686\n",
      "2020-07-15 09:27:36.665879 epoch [100/100], loss:0.001689, test_set_loss:0.001684\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss_sum=0.0; num=0\n",
    "    for inputs, in dataloader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        loss_sum+=loss.item()\n",
    "        num+=(inputs.shape[0]*inputs.shape[1])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1)%5 == 0:\n",
    "        print('{} epoch [{}/{}], loss:{:.6f}, test_set_loss:{:.6f}'\n",
    "                .format(datetime.now(), epoch + 1, num_epochs, loss_sum/num, test()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "To evaluation the predictive capability of the trained model, we do the following:\n",
    "\n",
    "* Preprocess the records using the MinMaxScaler (the *scaler* variable)\n",
    "* Use the preprocessed data as the input vectors of the model, and compute the output vectors by feed forward.\n",
    "* Calculate the root square of the input and output vectors, i.e. the generation losses.\n",
    "* For those generation losses that are greater than a predefined threshold, we score them as *fraud*; otherwise, score as *normal*.\n",
    "\n",
    "The value of the threshold is calculated via the distribution of the generation losses. First lets show the generation losses of the trained model for the *normal* and *fraud* cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of generation losses\n",
    "\n",
    "As the dataset is highly unbalanced, we take the whole positive (fraud) records while randomly choosing the equal number of negative (normal) records. The diagram below shows the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAHrCAYAAAAAK2ErAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAHRFJREFUeJzt3X+QpHldH/D3wiIXwCtHrCwYCKMeFFLmrnYJFdiICW5RoCEip35THGWIObUgUrViAqtSqFXG4IY/wh7IbwxF5AjfQmKIQuF5FczqKlWw6x3hhLsyjAEvIOoeHi57et7mj5495mZ6dqfn0/1099zrVbV13d9++vl++tPfmXvPM08/s+/ChQsBAAB27yHzLgAAAJadUA0AAEVCNQAAFAnVAABQJFQDAECRUA0AAEVCNQAAFAnVAABQJFQDAECRUA0AAEVCNQAAFAnVAABQJFQDAEDR/uoOWmtPSHIiyXck+UqS9yb5qd77Pa21H07ytk1P+XDv/bnVeQEAYFGUQnVr7WFJPpTkE0m+Pcljkrw7yd1JfjbJgSQfTHL9hqfdM+k8N99884VKnQAAMIkjR47sm2T76pHqZyS5KsnTeu9/leS21toNSb4/Xw3Vn+29f744DwAALKxqqP5MkhesB+qLzif5m/XbB5J8qjjH/Q4dOjStXc3U6dOnkyxPvctOv4el38PR62Hp93D0elj6PZmL/ZrUvgsXpndmRWvtIUk+luQ9vffXttY+kuTKJF+f5L6Mzrd+de/93kn2u2ynf5w7dy5J8ohHPGLOlTw46Pew9Hs4ej0s/R6OXg9Lv3dn6NM/NvuZJF+X5A3r9z+6Pse7kjwpyZuTfDnJL0x5XgAAmJupherW2guSvDLJd/Tev5IkvfdjGza5pbX2+CQvyS5D9bL82sKvWYal38PS7+Ho9bD0ezh6PSz9nsxuT/+YynWqW2vXZHQ0+kd67x+7xKa3JXncNOYEAIBFMY3rVB9I8oEkr+u9v3vD+KMz+pDiNb33O9eHr0py59a9AADA8qpep/rhSX4tye1J3thae8yGh/8iya1J3tRa+8mMrmH9k9n6x2AAAGCpVY9UP339X7L1CPSzkrwoyeuTnEryV0nek+Q1xTkBAGChlEJ17/23k1zuciM/UJkDAAAW3VQ+qAgAAA9mQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABSV/kw5AADLb+XE6tjxs0fXBq1jmTlSDQAARUI1AAAUCdUAAFAkVAMAQJFQDQAARUI1AAAUCdUAAFAkVAMAQJFQDQAARUI1AAAUCdUAAFAkVAMAQJFQDQAARUI1AAAUCdUAAFAkVAMAQJFQDQAARUI1AAAUCdUAAFAkVAMAQJFQDQAARUI1AAAUCdUAAFAkVAMAQJFQDQAARUI1AAAUCdUAAFAkVAMAQJFQDQAARUI1AAAUCdUAAFAkVAMAQJFQDQAARUI1AAAU7Z93AQAALKaVE6tbxs4eXRu8jmXgSDUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABTtr+6gtfaEJCeSfEeSryR5b5Kf6r3f01o7mOQtSa5JcmeSn+29v6s6JwAALJLSkerW2sOSfCjJPUm+PckPJnlhkp9urX1Nkg8k+XhGofq1SX65tXZNqWIAAFgw1SPVz0hyVZKn9d7/KsltrbUbknx/kt9JspLk5b3380k+1VprGYXuW4rzAgDAwqiG6s8kecF6oL7ofJK/SfKUJHesB+qLbk3y5N1Odvr06d0+dVDnzp1Lsjz1Ljv9HpZ+D0evh6Xfw9HrYe2k30cm2J/3bbxSqO69fzbJZy/eb609JKNTQN6T5MokX9r0lLNJvq0yJwAALJryBxU3+ZkkX5fkDUmOjXn8QgrncR86dGi3Tx3UxZ/glqXeZaffw9Lv4ej1sPR7OHo9rB31++TO97fX37fdHomf2iX1WmsvSPLKJK33/pUk943Z/74k905rTgAAWARTCdXrV/R4V5If6b1/bH347ow+qLjRSpK7pjEnAAAsinKobq0dyOjSea/rvb97w0O3JXlia+3KDWNXJ/lkdU4AAFgkpXOqW2sPT/JrSW5P8sbW2mM2PPyRJH+W5PWttdckeVaSw0leWpkTAAAWTfWDik9f/5eM/mLiRs9K8vwkb87outR/kuTFvfc/LM4JAAALpXpJvd/O6MOHl/K0yhwAALDopnb1DwAAeLASqgEAoEioBgCAIqEaAACKhGoAACgSqgEAoEioBgCAIqEaAACKhGoAACgSqgEAoKj0Z8qBB5eDx0/df/vMoTkWAgALxpFqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAivbPuwAAAIazcmJ13iXsSY5UAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAULR/3gUAX3Xw+Kn7b585dniOlQAAk3CkGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgKL98y4AHmwOHj91/+0zxw7PsZLL21hr5XmL/joZzsqJ1S1jZ4+uDV4HwLQ5Ug0AAEVCNQAAFAnVAABQJFQDAECRUA0AAEVCNQAAFAnVAABQJFQDAECRUA0AAEVCNQAAFAnVAABQJFQDAEDR/mnspLX22CQ/muQ5vffDG8Z/OMnbNm3+4d77c6cxLwAALIJyqG6tvSXJDyW5K8m5TQ8fSPLBJNdvGLunOicAACySaRyp/tMkT09ydZKf2/TYgSSf7b1/fgrzAADAQiqfU917f3Xv/fQ2Dx9I8oXqHAAAsMimck71JRxI8sTW2ouT3JfkvUle3Xu/dzc7O316u+y+WM6dG50Fsyz1Lrtl7velal701zNJfYv+WhbVMq/t7RwZM7Yor28v9ntR6fWwNvd73NfhJLxv4806VH90fY53JXlSkjcn+XKSX5jxvMAuXH/T+XmXwBI6cvLaseM3P/P9A1cCMD8zDdW992Mb7t7SWnt8kpdkl6H60KFDU6lr1i7+BLcs9S67pev3Tafuv7ml5ks9NoQN81/OJevbtJ+leW8WzNKs7ZPjh8fWPWbbRXl9S9PvPUCvh7Wl39t8ze7UXn/fdnskfujrVN+W5HEDzwkAADM1syPVrbVHJ/lUkmt673euD1+V5M7tnwUAAMtnZkeqe+9/nuTWJG9qrX1ra+1ZSX4yyX+Z1ZwAADAPsz7940VJ/jrJqYzC9HuSvGbGcwIAwKCmdvpH7/2dSd65aezzSX5gWnMAAMAiGvqDigAAsOcI1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUDS1P1MOjHfw+KmZ7OfMscMzfR6weysnVreMnT26NngdwHAcqQYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBo/7wLgL3m4PFTg89z5tjhqexnKNOqHZivlROrW8bOHl0bvA6G5X0fz5FqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAo2j/vAmAvOHj81FLsc14u9Vo2P3bm2OFZl8MSWzmxumXs7NG1weuYpXGvMdl7rxP2GkeqAQCgSKgGAIAioRoAAIqEagAAKBKqAQCgSKgGAIAioRoAAIqEagAAKBKqAQCgSKgGAIAioRoAAIqEagAAKBKqAQCgSKgGAIAioRoAAIqEagAAKNo/7wJglg4eP/WA+2eOHZ5TJezUxvfM+8VesnJidcvY2aNrU39+dZ5lsYivcxFrYjiOVAMAQJFQDQAARUI1AAAUCdUAAFAkVAMAQJFQDQAARUI1AAAUCdUAAFAkVAMAQJFQDQAARUI1AAAU7Z/GTlprj03yo0me03s/vGH8YJK3JLkmyZ1Jfrb3/q5pzAkAAIuifKS6tfaWJH+c5MeSfOOG8a9J8oEkH88oVL82yS+31q6pzgkAAItkGqd//GmSpyd55abxf5JkJcnLe++f6r2/McnvJHnhFOYEAICFUT79o/f+6iRprV296aGnJLmj935+w9itSZ5cnRMAABbJVM6p3saVSb60aexskm/b7Q5Pnz5dKmgo586dS7I89S67Sfq9aO/JtOqZZD/zmHOR51hky/K95Mg24+PqHrftJK+v+vxLmWa/t+vJZjvtUfX5s1pDu51rGr0e8nXu1CLWlGzt9yRrbKcW4XXO2yxD9UPHjF2IK46wB1x/0/nLbzSgzfW849lXzKmS2XqwvM694sjJa3e93c3PfP+0y5mJnb7GZfJgeT+W5TWxPGYZqu/L1gC9L8m9u93hoUOHSgUN5eJPa8tS77K7ZL9vOvWAu1N7Tzbtd7e21DOL/V6uB7uc85K9HKg/e/1rbGm+l5yc/i63fc1j5ppWf3bV7+JrHzvXBPvc6fOrr2na78dU1vYsel81w/VZsaXfQ37NLqHdHnWf5VHjuzP6oOJGK0numuGcAAAwuFmG6tuSPLG1duWGsauTfHKGcwIAwOBmefrHR5L8WZLXt9Zek+RZSQ4neekM5wQAgMHN7Eh17/2eJM/P6NJ6tyR5RZIX997/cFZzAgDAPEztSHXv/Z1J3rlp7ONJnjatOQAAYBG5vB0AABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUDS1P1MO83L9TedHN246lSQ5c+zwHKuZzMHjp+Zdwp61ubc7XRe7fR7TtXJidd4lwFSMW8tnj64NXgez50g1AAAUCdUAAFAkVAMAQJFQDQAARUI1AAAUCdUAAFAkVAMAQJFQDQAARUI1AAAUCdUAAFAkVAMAQJFQDQAARUI1AAAUCdUAAFAkVAMAQJFQDQAARUI1AAAU7Z93AcD0HTx+alePTWuORbSx3jPHDs+xkp07cvLa0Y2TXx07e3RtLrVctHJida7zz9Myv/Zlrh2WhSPVAABQJFQDAECRUA0AAEVCNQAAFAnVAABQJFQDAECRUA0AAEVCNQAAFAnVAABQJFQDAECRUA0AAEVCNQAAFAnVAABQJFQDAECRUA0AAEVCNQAAFO2fdwEwbQePn5rK884cOzyNcgBmZuXE6rxL2JHNdR5JcvMz3z+XWmBWHKkGAIAioRoAAIqEagAAKBKqAQCgSKgGAIAioRoAAIqEagAAKBKqAQCgSKgGAIAioRoAAIqEagAAKBKqAQCgSKgGAIAioRoAAIqEagAAKBKqAQCgaP+8C4DdOHj81LxLWHprV1y3ZWz1/I1zqASmb+XE6paxs0fXLrntkYsDJ7fflr1jkjUCO+FINQAAFAnVAABQJFQDAECRUA0AAEVCNQAAFAnVAABQJFQDAECRUA0AAEVCNQAAFAnVAABQJFQDAECRUA0AAEX7Z7nz1tpVSe7YNHxP7/2KWc4LAABDmmmoTnIgyV1JvnXD2IUZzwkAAIMaIlR/off++RnPAwAAczNIqJ7Wzk6fPj2tXc3UuXPnkixPvQ8mk7wn3r/5u9x7sNv3aFnWwZExY/Nel+NqmrdxPZmkdzvddhavfah5qmbRu3PnzpXW81DvxyR2WtPQX8ebc8ki9m4vGCJUP6G1dmuSb0hyMsnLeu9fnPG8DOj6m87ff/sdz57N6fIb55iHec8/ztoV120ZWz1/4xwqmZ8h1t5O5x9Xw5GT1+54Xzc/8/1TqenBaJI+A8zKrEP17Ul+L8kbkjw0yYkkNyZ59m52dujQoelVNkMXf1pblnrLbjp1/82ZveYNc1Rcsr5Nczxg2ynNz2S2vF+XeB9K217ieaVtT27/1B3VNOb5c/++MsFrWjTb9m6nfZ7Bax9qnqpZ9O4Rj3hEbT0P9X5MYoc1Df11vCWXLGLvFshuj7rPNFT33m/MKEQnSVpr1yf5eGvtG3vvd85ybgAAGMrQ16m+bf2/jxt4XgAAmJmZhurW2snW2g9sGLpq/b+OUgMAsGfM+pzq30zy71trf5Lk7iQ3JPlI7/1zM54XAAAGM+tQ/Zokj0ryviRXJPmfSV424zkBAGBQs/6g4r1Jjq3/AwCAPWnoDyoCAMCeI1QDAECRUA0AAEVCNQAAFAnVAABQJFQDAECRUA0AAEVCNQAAFAnVAABQJFQDAEDRTP9MOSy6g8dP7eoxkrUrrtsytnr+xqnOMdR7sAjrYOXE6lS3S5KzR9d2Vctu5loGe+31LKpZ9Nl7t3srJ1Zz5OKdk/OsZO9zpBoAAIqEagAAKBKqAQCgSKgGAIAioRoAAIqEagAAKBKqAQCgSKgGAIAioRoAAIqEagAAKBKqAQCgSKgGAIAioRoAAIqEagAAKBKqAQCgSKgGAICi/fMugL3l4PFT2z525tjhXW87LZeak+2tXXHdVLfbzur5G3e/zxPjn79T281T2ee8rZxY3TJ29uja4HUso3G9W+Z5YNZ8v3GkGgAAyoRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgKL98y6AxXTw+Kn7b585dniu809q7Yrrxo6vnr9x18/f6XMneX61Trbarqebnc3abOY+MfXdzszmr7F5fJ2zPFZOrE59n0dOXpucnPpuWVLj1tjZo2uD11HhSDUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABFQjUAABQJ1QAAUCRUAwBAkVANAABF++ddwKI7ePzUA+6fOXZ4TpWMN0l9m7edxRyXsnJi9QH3165IVs/fuKt9LYq1K65bqH1W65nF61nE+Sf5WhhX0zKv24tfh2tXPHD8bNYGrwUerDb///Cis0fXBq2D6XKkGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgCKhGgAAioRqAAAoEqoBAKBIqAYAgKL9s56gtfboJG9N8twk55O8M8kreu/3zXpuAAAYwsxDdZI3JXlsksNJHpPkV5L8cZIbBpgbAABmbqanf7TWHpnke5O8svd+S+/9w0l+Kcl1s5wXAACGNOtzqq9K8rAkn9gwdmuSJ894XgAAGMy+CxcuzGznrbVnJvlfvfd9G8aeleTm3vuOA/3NN988uyIBAGCTI0eO7Lv8Vl816yPVDx0zdiHJREUCAMAim/UHFe9LktbaQzZc7WNfkr+dZCeT/qQAAABDmvWR6rvX/7uyYWwlyV0znhcAAAYz61B9R5J7kzx1w9jVST4543kBAGAwM/2gYpK01t6X5JuTXJ/kG5LcmORVvfe3znRiAAAYyBB//OUlSd6W5HeTnEvy9vX7AACwJ8z8SDUAAOx1sz6nGgAA9jyhGgAAioRqAAAoEqoBAKBIqAYAgKIhLqm357XWrk7yn5L8wyRnk7y19/4fLrH955L8vU3Dz+i9//7sqtw7Jul3a21fkl9M8q+TPDLJTUl+uPf+xYHKXXqttW9J8m+SfFPv/drLbGttF+2039Z2TWvtmzK6vOs/zuj7yOt67//xEttb2xNqrT06yVuTPDfJ+STvTPKK3vt9Y7b9iST/Nsmjk/x+kut77380XLXLb8J+/05Ga3+jF/be/+us69zLHKkuaq19fZLfSvK/kzwtyY8neVVr7Ye22X5fkr+b5DuTPHbDv48PUvCSm7TfSV6a5F8leWGSw0kek+Qts690b2it/UaSTyf5l0m+/jLbWttFk/Q71nbVe5PcndFf/H15kp9rrX3PuA2t7V17U0Z9OpzkuozW9cs2b9Ra++4kP5/kaEbvx18m6cOVuWfsqN/rDiR5cR64nn9tgBr3NEeq656X5J4kL1//afD21tqvrI//5zHbryR5WJI/7L1/frgy94xJ+/2iJK/vvf9WkrTWjiW5qbX2qN77l4cqeondnlHguC7JP73MttZ23ST9trZ3qbX2xIx+KP++3vtnk9zWWvuujPr+gTFPsbYn1Fp7ZJLvTfKdvfdbktzSWvuljHp8w6bNX5TkPb33960/92iS/9Nae3Lv/VND1r2sJux3MgrVt1vP0+VIdd1Hk/zgpl+vnE/yN9tsfyDJfUn8inZ3Ju33U5J8YsP9WzP6YfKq2ZS3t/TeX957v32Hm1vbRRP229revackuWs9UF90a5Inb7O9tT25qzL6QWTzGh3X4wes5d77ZzL6LcJ27wdb7bjfrbW/k+Rrk3xhmNIePBypLuq9fzqjX9cmuf+nxe/L6GjTOAeS3Jvk5tbaU5J8KsmP995Pz7rWvWAX/b4yyZc23D+7YZzpsraHZW3v3ubeJaP+bdc7a3tyVyZJ733zGh3X40nfD7aapN8H1v/79tbaP0jyx0l+uvd+02xL3PuE6h1qrT08yb5Nw/f13v9609gbkvzfJO/bZldnM/pA0VuTfC7JTyT5zdbaN/fe/3KKJS+1Kfb7Ab+N6b1faK1tGX8wm6DXl2Nt78AU+21tX8Z2vU7y0DGbX8j2vbO2J7ddjze/H5fa1lreuUn6fW+SDyZ5d5Lbkvxgkl9fP93mM7Mrce8Tqnfu00mesGnso0mefvHO+nlg35Pkqb33C+N2sn6u0/M2POf6jL5JPy/JjVOueZlNpd/Z9I15/QNHyeibCiOX7fVOWNs7NpV+x9reie16/cZsDWz7sk3vrO1duS9JWmsP2XC63r4kf7vNtjt+Pxhrx/3uvX8uyT/bMPQHrbVnZ/Sh522vXMblCdU71HtfvdTjrbXnZHR5q+/qva9NsN97Wmt/lORxpQL3mCn2++6MPmR00cXbd1Xq20su1+vCfq3tMabYb2v7MrbrdWvtBXlg77J+f0e9s7Z35O71/64k+fMNt8f1ePNavtS2jDdJv8e5LdZzmV+tTEFr7VszujzTy3rvH7nMtr/YWnvdhvsPTbKa5M5Z1riXTNLvjL5RPHXD/aszunqI659OmbU9OGt7925L8qjW2pM2jF2d5JPjNra2d+WOjI40b16j43r8gLW8fg3xr91mW8bbcb9baz/WWvvVTcNXxXouc6S6aP26yf8jyX9L8huttcdsePiLvfe/ba09vPd+z/rYh5J8cP3C63+Q0TUkH5bR+U1cxi76/e4kP7/e7z9JcjzJ+3vvXxm08D3K2h6WtT0dvfdPt9Y+nuSG1tq/y+jqE/8iyfMvbmNt1/Tev9xa++9JfrG19sUk35Dkx5K8Khm7ln+1tfaiJGcy+i3k7zm/d+cm7PdNSV7bWnvJ+u3vyyiAv3D4yvcWR6rr/nmSb8nojzD8v03/Ht9ae0ZG19t8eJL03n87yUuS/EJG35z/UUanMPzF8KUvpYn6ndG5k29P8q4kv5dR+NjuYvhMwNoelrU9dS3J1yT5WJLXZXT1g99MrO0peklGV5b43STvSfKOJG9rrf39JJ9rrT0+SXrvv55R+HttktMZ/YXQ6+ZS8XLbab9vzyhIvzSjy+69KMnze+93zKXqPWTfhQvbfb4LAADYCUeqAQCgSKgGAIAioRoAAIqEagAAKBKqAQCgSKgGAIAioRoAAIqEagAAKBKqAQCgSKgGAIAioRoAAIqEagAAKBKqAQCgSKgGAIAioRoAAIqEagAAKPr/jFOZNJKySIUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb06992ab38>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_set2 = data[data.Class==-1][feature_names]\n",
    "    test_set2=scaler.transform(test_set2)\n",
    "    inputs2=torch.tensor(test_set2, dtype=torch.float32)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs2.to('cuda')\n",
    "    outputs2=model(inputs2)\n",
    "    loss2=torch.sum((inputs2-outputs2)**2,dim=1).sqrt().log()\n",
    "\n",
    "    test_set1=test_set[np.random.choice(len(test_set),size=len(loss2),replace=False)]\n",
    "    inputs1=torch.tensor(test_set1, dtype=torch.float32)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs1.to('cuda')\n",
    "    outputs1=model(inputs1)\n",
    "    loss1=torch.sum((inputs1-outputs1)**2,dim=1).sqrt().log()\n",
    "\n",
    "    pd.Series(loss1.numpy()).hist(bins=100)\n",
    "    pd.Series(loss2.numpy()).hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of threshold (*split point*)\n",
    "\n",
    "If the distribution functions of the generation losses are normal, the threshold is easy to calculate. It is the  intersection of the positive-case and negative-case distributions. However, as we see from the diagram above, neither of the distribution function is strictly normal.\n",
    "\n",
    "Here, we define the threshold as the split point that maximizse the average pecision in both positive and negative cases. And use a *5-points* heuristic search approach to find the split point (see the **find_split_point** function).\n",
    "\n",
    "Initially, we define the search range as the minimum generation loss of the positive cases and the maximum of the negative cases. For each iteration, we pick up half of the search range using the heuristic function to evaluate if the picked range contains the split point. When the search range is smaller than 0.01, the iterations stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.1253912448883057 -> -0.6022694706916809\n",
      "-1.3638303577899933 -> -0.6022694706916809\n",
      "-1.1734401360154152 -> -0.792659692466259\n",
      "-1.1734401360154152 -> -0.9830499142408371\n",
      "-1.1258425805717707 -> -1.0306474696844816\n",
      "-1.1020438028499484 -> -1.0544462474063039\n",
      "-1.1020438028499484 -> -1.0782450251281261\n",
      "-1.0960941084194928 -> -1.0841947195585817\n",
      "-1.0901444139890373 -> -1.0841947195585817\n",
      "\n",
      "Split point: -1.0901444139890373\n"
     ]
    }
   ],
   "source": [
    "def precision_rate(split_point):\n",
    "    rate1=(loss1<split_point).sum().item()/float(len(loss1))\n",
    "    rate2=(loss2>split_point).sum().item()/float(len(loss2))\n",
    "    return (rate1+rate2)/2            \n",
    "\n",
    "def find_split_point(start,end,start_precision,end_precision):\n",
    "    print(start,'->',end)\n",
    "    delta=(end-start)/4.0\n",
    "    precision=[start_precision]\n",
    "    precision+=[precision_rate(start+i*delta) for i in range(1,4)]\n",
    "    precision+=[end_precision]\n",
    "\n",
    "    i = 0 if sum(precision[0:3])>sum(precision[1:4]) else 1\n",
    "    j = i if sum(precision[i:i+3])>sum(precision[2:5]) else 2\n",
    "\n",
    "    if end-start>0.01:\n",
    "        return find_split_point(start+j*delta,start+(j+2)*delta,precision[j],precision[j+2])\n",
    "    else:\n",
    "        return start+delta*np.argmax(precision)\n",
    "\n",
    "\n",
    "(start,end)=(loss1.max().item(),loss2.min().item())\n",
    "(start,end)=(start,end) if start<end else (end,start)\n",
    "split_point=find_split_point(start,end,precision_rate(start),precision_rate(end))\n",
    "print('\\nSplit point:',split_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the outputs above, we can see how the search range shrinks with each iterations, and the final split point calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model precision rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision rate for normal cases: 0.959349593495935\n",
      "Precision rate for fraud cases: 0.8800813008130082\n",
      "Overall precision: 0.9197154471544715\n"
     ]
    }
   ],
   "source": [
    "precision1=(loss1<split_point).sum().item()/float(len(loss1))\n",
    "precision2=(loss2>split_point).sum().item()/float(len(loss2))\n",
    "\n",
    "print('Precision rate for normal cases:',precision1)\n",
    "print('Precision rate for fraud cases:',precision2)\n",
    "print('Overall precision:',(precision1+precision2)/2)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
