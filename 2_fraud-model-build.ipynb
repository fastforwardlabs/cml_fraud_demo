{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Model Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores **building the fraud detection model**.\n",
    "This model will learn what \"normal\" transactions are like, and will judge whether a new transaction is different enough from a \"normal\" one that we should consider it fraudulent.\n",
    "\n",
    "The following work will look fairly standard to anyone having trained machine learning models using python Jupyter notebooks.\n",
    "The CML platform provides a **fully capable Jupyter notebook environment** that data scientists know and love.\n",
    "\n",
    "In this part, you will learn how to train ML models using pandas and both scikit-learn's OneClassSVM and an autoencoder using Pytorch. For more detailed information on that approaches used here, please see Cloudera Fast Forward's [Deep Learning for Anomaly Detection](https://ff12.fastforwardlabs.com/) report.\n",
    "\n",
    "If you haven't yet, run through the initialization steps in the README file and Part 1. \n",
    "In Part 1, the data is imported into the `default.cc_data` table in Hive. \n",
    "All data accesses fetch from Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Kaggle [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset here. The datasets contains transactions made by credit cards in September 2013 by European cardholders, and presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "The dataset should be uploaded to hive in the step 1. We will first load it as a Pandas DataFrame via Spark, and check the number of positive and negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: decimal(10,0) (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonSQL\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    spark_df = spark.sql(\"SELECT * FROM default.cc_data\")\n",
    "    spark_df.printSchema()\n",
    "    data = spark_df.toPandas()\n",
    "except:\n",
    "    data = pd.read_csv(\"/home/cdsw/data/creditcardfraud.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 284807\n",
      "Positive samples: 284315\n",
      "Negative samples: 492\n"
     ]
    }
   ],
   "source": [
    "print('Number of records:',len(data))\n",
    "print('Positive samples:',len(data[data.Class==0]))\n",
    "print('Negative samples:',len(data[data.Class==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains only numerical input variables which are the result of a PCA transformation. Features V1, V2, … V28 are the principal components obtained using a PCA process, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, and the feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0    0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1    0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2    1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3    1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4    2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Choosing positive samples\n",
    "\n",
    "The next step is to munge the data for later steps. \n",
    "As we saw, this dataset has already been preprocessed to be very clean, and entirely numeric.\n",
    "The only column that needs cleaning is `Class`.\n",
    "For anomaly detection, scikit-learn marks inliers as `1`, and outliers as `-1`,\n",
    "whereas this dataset has marked them `0` and `1`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Class'] = data['Class'].replace({0: 1, 1: -1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoder for fraud detection uses only the positive (non-fraud) samples to train the model, thus only chooses records with Class==0. Then we split the chosen records as train set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In class training set: 284315\n"
     ]
    }
   ],
   "source": [
    "in_class_set = data[data.Class==1]\n",
    "out_class_set = data[data.Class==-1]\n",
    "print('In class training set:',len(in_class_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Class SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the main model used in this project is a deep learning model, there are other techniques that can be used for anomaly detection. Here we show how to use a scikit-learn OneClassSVM.\n",
    "\n",
    "This step follows a fairly standard ML workflow. Here, we:\n",
    "- normalize the numeric features\n",
    "- split into *test* and *train* sets\n",
    "- train a classification model using these processed features\n",
    "- evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For anomaly detection, we train the model *only* on inliers so it learns what normal data look like.\n",
    "We then test on both inliers and outliers, so splitting the *test* and *train* sets will be slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test/train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(in_class_set.drop('Class', axis=1),\n",
    "                                                     in_class_set['Class'],\n",
    "                                                     test_size=len(out_class_set),  # balanced test set\n",
    "                                                     random_state=42)\n",
    "# add outliers to test set\n",
    "X_test = X_test.append(out_class_set.drop('Class', axis=1))\n",
    "y_test = y_test.append(out_class_set['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features based on test set\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneClassSVM(gamma=0.5, nu=0.01, verbose=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train one-class SVM\n",
    "clf = OneClassSVM(gamma=0.5, nu=0.01, verbose=True)\n",
    "clf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9900008103642058\n",
      "test accuracy: 0.8404471544715447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.69      0.81       492\n",
      "           1       0.76      0.99      0.86       492\n",
      "\n",
      "    accuracy                           0.84       984\n",
      "   macro avg       0.87      0.84      0.84       984\n",
      "weighted avg       0.87      0.84      0.84       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate one-class SVM\n",
    "print('train accuracy:', accuracy_score(y_train, clf.predict(X_train)))\n",
    "print('test accuracy:', accuracy_score(y_test, clf.predict(X_test)))\n",
    "print(classification_report(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa417570ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ocsvm_auc = roc_auc_score(y_test, clf.score_samples(X_test))\n",
    "fpr, tpr, _ = roc_curve(y_test, clf.score_samples(X_test))\n",
    "plt.plot(fpr, tpr, label='OCSVM')\n",
    "plt.title('OCSVM: AUROC=%.3f' % (ocsvm_auc))\n",
    "plt.ylabel('true positive rate')\n",
    "plt.xlabel('false positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section shows how we can train a deep learning model using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the model, we will use the full dataset and not a balanced set like we did for the OneClassSVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "feature_names=data.columns.values[:-1]\n",
    "train_test_set = in_class_set[feature_names]\n",
    "train_set, test_set = train_test_split(train_test_set, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.42327013e-01, 9.47416596e-01, 7.97217483e-01, 8.81816356e-01,\n",
       "        2.41740662e-01, 5.43571003e-01, 5.15661441e-01, 4.19171267e-01,\n",
       "        8.02040109e-01, 4.16891659e-01, 4.47903682e-01, 2.35464057e-01,\n",
       "        6.15377573e-01, 4.10107015e-01, 5.23937212e-01, 4.48405051e-01,\n",
       "        6.02247154e-01, 6.71423155e-01, 6.12703253e-01, 5.13449227e-01,\n",
       "        4.11165178e-01, 6.10928994e-01, 5.39959714e-01, 6.67924552e-01,\n",
       "        5.06573308e-01, 5.33984885e-01, 5.03033612e-01, 4.65364700e-01,\n",
       "        2.22235356e-01, 5.09856012e-03],\n",
       "       [2.44658194e-01, 9.79156142e-01, 7.97062325e-01, 8.90512717e-01,\n",
       "        3.03888114e-01, 5.48372609e-01, 5.15350500e-01, 4.21609233e-01,\n",
       "        7.95745386e-01, 3.66345976e-01, 4.95729107e-01, 3.75257082e-01,\n",
       "        6.96864586e-01, 5.41487510e-01, 5.84645532e-01, 3.64243237e-01,\n",
       "        5.45108394e-01, 6.30551391e-01, 5.31712561e-01, 5.85659779e-01,\n",
       "        4.13125001e-01, 6.06201154e-01, 5.15266633e-01, 6.62486089e-01,\n",
       "        4.14903779e-01, 6.25814080e-01, 3.86877333e-01, 4.70760274e-01,\n",
       "        2.21269048e-01, 5.08736791e-05]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler=MinMaxScaler().fit(train_set)\n",
    "train_set=scaler.transform(train_set)\n",
    "test_set=scaler.transform(test_set)\n",
    "train_set[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the distribution of each feature. The following diagram shows the distribution of the first 4 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa40ee209e8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa40ee21358>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.Series(train_set[:,0]).hist(bins=100)\n",
    "pd.Series(train_set[:,1]).hist(bins=100)\n",
    "pd.Series(train_set[:,2]).hist(bins=100)\n",
    "pd.Series(train_set[:,3]).hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "Here we define the autoencoder model for PyTorch. There are 2 hidden layers each for the encoder and decoder, with 15 and 7 cells respectively, and ReLU is used as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self,num_input):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(num_input, 15),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(15, 7))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(7, 15),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(15, num_input),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this process work with both CPU and GPU sessions, the GPU specific requirements are put after a `if torch.cuda.is_available():` check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# pytorch parallelism does not work as expected when using GPU.\n",
    "if torch.cuda.is_available():\n",
    "    num_workers = 4\n",
    "else:\n",
    "    num_workers = 0\n",
    "\n",
    "inputs = torch.tensor(train_set, dtype=torch.float32)\n",
    "dataset = TensorDataset(inputs)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "\n",
    "model = autoencoder(inputs.shape[1])\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda') \n",
    "    criterion.to('cuda')\n",
    "    inputs.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining test function\n",
    "\n",
    "The test function is used for cross validation during the training. Here we choose to do cross validation for each 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tests_=torch.tensor(test_set, dtype=torch.float32)\n",
    "        if torch.cuda.is_available():\n",
    "            tests_.to('cuda')\n",
    "        outputs = model(tests_)\n",
    "        loss=criterion(outputs,tests_)\n",
    "    return loss.item()/(tests_.shape[0]*tests_.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-14 10:00:43.743835 epoch [5/100], loss:0.001049, test_set_loss:0.001054\n",
      "2020-07-14 10:00:55.904853 epoch [10/100], loss:0.001014, test_set_loss:0.001035\n",
      "2020-07-14 10:01:08.707484 epoch [15/100], loss:0.000975, test_set_loss:0.000989\n",
      "2020-07-14 10:01:20.207972 epoch [20/100], loss:0.000969, test_set_loss:0.000973\n",
      "2020-07-14 10:01:33.647457 epoch [25/100], loss:0.000951, test_set_loss:0.000953\n",
      "2020-07-14 10:01:47.955458 epoch [30/100], loss:0.000947, test_set_loss:0.000944\n",
      "2020-07-14 10:02:00.167221 epoch [35/100], loss:0.000940, test_set_loss:0.000942\n",
      "2020-07-14 10:02:12.153231 epoch [40/100], loss:0.001428, test_set_loss:0.001558\n",
      "2020-07-14 10:02:26.195039 epoch [45/100], loss:0.001036, test_set_loss:0.001032\n",
      "2020-07-14 10:02:40.383655 epoch [50/100], loss:0.000969, test_set_loss:0.000964\n",
      "2020-07-14 10:02:53.045303 epoch [55/100], loss:0.000963, test_set_loss:0.000963\n",
      "2020-07-14 10:03:04.511626 epoch [60/100], loss:0.000962, test_set_loss:0.000974\n",
      "2020-07-14 10:03:17.804840 epoch [65/100], loss:0.000976, test_set_loss:0.000985\n",
      "2020-07-14 10:03:32.162251 epoch [70/100], loss:0.001013, test_set_loss:0.000980\n",
      "2020-07-14 10:03:45.419330 epoch [75/100], loss:0.000975, test_set_loss:0.000971\n",
      "2020-07-14 10:03:57.643276 epoch [80/100], loss:0.001031, test_set_loss:0.000971\n",
      "2020-07-14 10:04:10.996980 epoch [85/100], loss:0.000974, test_set_loss:0.000972\n",
      "2020-07-14 10:04:24.576845 epoch [90/100], loss:0.000967, test_set_loss:0.000966\n",
      "2020-07-14 10:04:38.449482 epoch [95/100], loss:0.000981, test_set_loss:0.000961\n",
      "2020-07-14 10:04:51.581976 epoch [100/100], loss:0.001248, test_set_loss:0.001235\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss_sum=0.0; num=0\n",
    "    for inputs, in dataloader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        loss_sum+=loss.item()\n",
    "        num+=(inputs.shape[0]*inputs.shape[1])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1)%5 == 0:\n",
    "        print('{} epoch [{}/{}], loss:{:.6f}, test_set_loss:{:.6f}'\n",
    "                .format(datetime.now(), epoch + 1, num_epochs, loss_sum/num, test()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "To evaluation the predictive capability of the trained model, we do the following:\n",
    "\n",
    "* Preprocess the records using the MinMaxScaler (the *scaler* variable)\n",
    "* Use the preprocessed data as the input vectors of the model, and compute the output vectors by feed forward.\n",
    "* Calculate the root square of the input and output vectors, i.e. the generation losses.\n",
    "* For those generation losses that are greater than a predefined threshold, we score them as *fraud*; otherwise, score as *normal*.\n",
    "\n",
    "The value of the threshold is calculated via the distribution of the generation losses. First lets show the generation losses of the trained model for the *normal* and *fraud* cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of generation losses\n",
    "\n",
    "As the dataset is highly unbalanced, we take the whole positive (fraud) records while randomly choosing the equal number of negative (normal) records. The diagram below shows the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa413a4e748>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_set2 = data[data.Class==-1][feature_names]\n",
    "    test_set2=scaler.transform(test_set2)\n",
    "    inputs2=torch.tensor(test_set2, dtype=torch.float32)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs2.to('cuda')\n",
    "    outputs2=model(inputs2)\n",
    "    loss2=torch.sum((inputs2-outputs2)**2,dim=1).sqrt().log()\n",
    "\n",
    "    test_set1=test_set[np.random.choice(len(test_set),size=len(loss2),replace=False)]\n",
    "    inputs1=torch.tensor(test_set1, dtype=torch.float32)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs1.to('cuda')\n",
    "    outputs1=model(inputs1)\n",
    "    loss1=torch.sum((inputs1-outputs1)**2,dim=1).sqrt().log()\n",
    "\n",
    "    pd.Series(loss1.numpy()).hist(bins=100)\n",
    "    pd.Series(loss2.numpy()).hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of threshold (*split point*)\n",
    "\n",
    "If the distribution functions of the generation losses are the convex function, the threshold is very easy to calculation. It is the just the intersection of the positive-case and negative-case distribution functions. However, as we see from the diagram above, neither of the distribution function is strictly convex.\n",
    "\n",
    "Here, we define the threshold as the split point that maximize the average precise in both positive and negative cases. And use a *5-points* heuristic search approach to find the split point (see the **find_split_point** function).\n",
    "\n",
    "Initially, we define the search range as the minimum generation loss of the positive cases and the maximum of the negative cases. For each iteration, we pick up half of the search range using the heuristic function to evaluate the picked range contains the split point. When the search range is smaller than 0.01, the iterations stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.201681137084961 -> -0.4309435188770294\n",
      "-1.3163123279809952 -> -0.4309435188770294\n",
      "-1.3163123279809952 -> -0.8736279234290123\n",
      "-1.2056412268429995 -> -0.984299024567008\n",
      "-1.1503056762740016 -> -1.0396345751360059\n",
      "-1.1503056762740016 -> -1.0949701257050037\n",
      "-1.1226379009895027 -> -1.0949701257050037\n",
      "-1.1088040133472532 -> -1.0949701257050037\n",
      "-1.1018870695261285 -> -1.0949701257050037\n",
      "\n",
      "Split point: -1.096699361660285\n"
     ]
    }
   ],
   "source": [
    "def precise_rate(split_point):\n",
    "    rate1=(loss1<split_point).sum().item()/float(len(loss1))\n",
    "    rate2=(loss2>split_point).sum().item()/float(len(loss2))\n",
    "    return (rate1+rate2)/2            \n",
    "\n",
    "def find_split_point(start,end,start_precise,end_precise):\n",
    "    print(start,'->',end)\n",
    "    delta=(end-start)/4.0\n",
    "    precise=[start_precise]\n",
    "    precise+=[precise_rate(start+i*delta) for i in range(1,4)]\n",
    "    precise+=[end_precise]\n",
    "\n",
    "    i = 0 if sum(precise[0:3])>sum(precise[1:4]) else 1\n",
    "    j = i if sum(precise[i:i+3])>sum(precise[2:5]) else 2\n",
    "\n",
    "    if end-start>0.01:\n",
    "        return find_split_point(start+j*delta,start+(j+2)*delta,precise[j],precise[j+2])\n",
    "    else:\n",
    "        return start+delta*np.argmax(precise)\n",
    "\n",
    "\n",
    "(start,end)=(loss1.max().item(),loss2.min().item())\n",
    "(start,end)=(start,end) if start<end else (end,start)\n",
    "split_point=find_split_point(start,end,precise_rate(start),precise_rate(end))\n",
    "print('\\nSplit point:',split_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the outputs above, we can see how the search range shrinks with each iterations, and the final split point calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model precision rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision rate for normal cases: 0.9735772357723578\n",
      "Precision rate for fraud cases: 0.8719512195121951\n",
      "Overall precision: 0.9227642276422765\n"
     ]
    }
   ],
   "source": [
    "precise1=(loss1<split_point).sum().item()/float(len(loss1))\n",
    "precise2=(loss2>split_point).sum().item()/float(len(loss2))\n",
    "\n",
    "print('Precision rate for normal cases:',precise1)\n",
    "print('Precision rate for fraud cases:',precise2)\n",
    "print('Overall precision:',(precise1+precise2)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'creditcard-fraud-2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
